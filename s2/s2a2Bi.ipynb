{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 -- Assignment 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# imported numpy package for data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the input and output in X and y, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 0, 1, 0], [1, 0, 1, 1], [0, 1, 0, 1]])\n",
    "y = np.array([[1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define weights (wh_1) for the 1st hidden layer that connects with the\n",
    "input layer and define the bias (bh_1) for the hidden layer. Both the \n",
    "weights and biases for the hidden layer were defined using random number\n",
    "generation method using numpy operation -- np.random.random()\n",
    "\n",
    "Before generating random numbers using np.random.random(), use the function\n",
    "np.random.seed() to ensure repeatability of the number generation.\n",
    "This will help us cross-check results with other systems, and is convenient\n",
    "to also manually calculate and cross-check with the results from scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "wh_1 = np.round(np.random.random((4, 3)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These bias values are sorted in the descending order (just to give higher values initially to the first two X values -- input, and attach low significance to the last input value -- to reflect the output y values [1 1 0]) though this is not needed -- just a guess, but the back propagation will adjust even if its a wrong guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "bh_1 = -np.sort(-np.round(np.random.random((1, 3)), 2), axis=-1)\n",
    "np.random.seed(1234)\n",
    "wout = np.random.random((3, 1))\n",
    "np.random.seed(1234)\n",
    "bout = np.random.random((1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate hidden layer input which is a biased weighted sum of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_input = np.dot(X, wh_1) + bh_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate activation function for the computed input\n",
    "First we will define the sigmoid function which is used in converting the \n",
    "hidden layer input to intermediate output which is then fed into the next layer\n",
    "as input. The sigmoid is one among many activation functions used in the \n",
    "Neural Network algorithms. The output from sigmoid is in the range of [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "hiddenlayer_activations = np.round(sigmoid(hidden_layer_input), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hidden layer activation goes as input to the next layer -- which is our\n",
    "final output layer in this case (we can have multiple intermediate hidden layers\n",
    "most often). Since this is our simple case study on how to implement backward\n",
    "propagation we have limited oursellves to deal with 3 layers i.e. one input layer,\n",
    "one hidden layer and one output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_layer_input =np.dot(hiddenlayer_activations, wout) + bout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally sigmoid of the above input is our final output for the first cycle,\n",
    "and we calculate error based on the known actual output and the output from the\n",
    "output layer, which is then used to backpropagate to fine tune the weights\n",
    "and biases so as to reduce the error in the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.round(sigmoid(outer_layer_input), 2)\n",
    "error_out = y - output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the slope at output and hidden layer.\n",
    "\n",
    "slope_output_layer = derivatives_sigmoid(output) = output*(1-output)\n",
    "\n",
    "slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "                   = hiddenlayer_activations(1-hiddenlayer_activations)\n",
    "\n",
    "also define the learning rate (lrng_rate), the scale for incrementing error \n",
    "values while updating the output at the previous layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrng_rate = 0.1\n",
    "slope_output_layer = np.round(output*(1-output), 2)\n",
    "slope_hidden_layer = np.round(hiddenlayer_activations*(1-hiddenlayer_activations), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the change or increment of the error value based on the slope\n",
    "computed above. This is called the error responsibility of the node from the \n",
    "input layer (or intermediate hidden layer that serves as input to the next layer).\n",
    "\n",
    "Error responsibility = (1-ouput)*output*(actual output - computed output) for \n",
    "the output layer.\n",
    "\n",
    "Error responsbility (for nodes in hidden/intermediate layers) is given by,\n",
    "error resp = output*(1-output)*Summation of Wjk*deltaj, where the \n",
    "summation of Wjk*deltaj represents the error responsbilities for nodes downstream\n",
    "from the particular hidden layer node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_output = np.round(slope_output_layer*error_out, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate error at hidden layer, the partial derivatives of error from the\n",
    "output is computed at several points -- like partial derivative (here after, \n",
    "p.d) of error w.r.t output layer output, p.d of output layer output to \n",
    "output layer input (p.d of activation function), p.d of output layer input\n",
    "to hidden layer output, p.d of hidden layer output to its input (p.d of \n",
    "activation function), p.d of hidden layer input to input layer weighted coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_hd_layer = np.dot(d_output, wout.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change or increment in the weights of hidden layer is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_hiddenlayer = np.round(slope_hidden_layer*error_hd_layer, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in the original or last updated values for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_wout = wout\n",
    "last_wh_1 = wh_1\n",
    "last_bh_1 = bh_1\n",
    "last_bout = bout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the change or increments in the respective weights/biases are added to\n",
    "their original or last updated weights/biases scaled by the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wout = wout + np.dot(hiddenlayer_activations.T, d_output)*lrng_rate\n",
    "wh_1 = wh_1 + np.dot(X.T, d_hiddenlayer)*lrng_rate\n",
    "\n",
    "bh_1 = bh_1 + np.sum(d_hiddenlayer, axis=0)*lrng_rate\n",
    "bout = bout + np.sum(d_output, axis=0)*lrng_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original or lastly updated hidden layer weights/biases are \n",
      " [[0.19 0.62 0.44]\n",
      " [0.79 0.78 0.27]\n",
      " [0.28 0.8  0.96]\n",
      " [0.88 0.36 0.5 ]] \n",
      " and \n",
      " [[0.62 0.44 0.19]]\n",
      "Original or lastly updated output layer weights/biases are \n",
      " [[0.19151945]\n",
      " [0.62210877]\n",
      " [0.43772774]] \n",
      " and \n",
      " [[0.19151945]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original or lastly updated hidden layer weights/biases are \\n\", last_wh_1,\"\\n and \\n\", last_bh_1)\n",
    "print(\"Original or lastly updated output layer weights/biases are \\n\", last_wout,\"\\n and \\n\", last_bout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
